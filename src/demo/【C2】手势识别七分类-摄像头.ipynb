{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手势识别七分类-摄像头\n",
    "\n",
    "同济子豪兄 https://space.bilibili.com/1900783\n",
    "\n",
    "2021-07-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File loading is not yet supported on Windows",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     11\u001b[0m options \u001b[39m=\u001b[39m vision\u001b[39m.\u001b[39mGestureRecognizerOptions(base_options\u001b[39m=\u001b[39mbase_options, \n\u001b[0;32m     12\u001b[0m                                           num_hands\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, \u001b[39m# 最多检测多少只手\u001b[39;00m\n\u001b[0;32m     13\u001b[0m                                          )\n\u001b[0;32m     15\u001b[0m \u001b[39m# def print_result(result, timestamp_ms):\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m#     pass\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# options = vision.GestureRecognizerOptions(base_options=base_options, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m#                                           result_callback=print_result\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m#                                          )\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m recognizer \u001b[39m=\u001b[39m vision\u001b[39m.\u001b[39;49mGestureRecognizer\u001b[39m.\u001b[39;49mcreate_from_options(options)\n",
      "File \u001b[1;32md:\\conda\\envs\\aaa\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\gesture_recognizer.py:296\u001b[0m, in \u001b[0;36mGestureRecognizer.create_from_options\u001b[1;34m(cls, options)\u001b[0m\n\u001b[0;32m    277\u001b[0m   options\u001b[39m.\u001b[39mresult_callback(gesture_recognizer_result, image,\n\u001b[0;32m    278\u001b[0m                           timestamp\u001b[39m.\u001b[39mvalue \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m _MICRO_SECONDS_PER_MILLISECOND)\n\u001b[0;32m    280\u001b[0m task_info \u001b[39m=\u001b[39m _TaskInfo(\n\u001b[0;32m    281\u001b[0m     task_graph\u001b[39m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[0;32m    282\u001b[0m     input_streams\u001b[39m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    294\u001b[0m     ],\n\u001b[0;32m    295\u001b[0m     task_options\u001b[39m=\u001b[39moptions)\n\u001b[1;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    297\u001b[0m     task_info\u001b[39m.\u001b[39;49mgenerate_graph_config(\n\u001b[0;32m    298\u001b[0m         enable_flow_limiting\u001b[39m=\u001b[39;49moptions\u001b[39m.\u001b[39;49mrunning_mode \u001b[39m==\u001b[39;49m\n\u001b[0;32m    299\u001b[0m         _RunningMode\u001b[39m.\u001b[39;49mLIVE_STREAM), options\u001b[39m.\u001b[39;49mrunning_mode,\n\u001b[0;32m    300\u001b[0m     packets_callback \u001b[39mif\u001b[39;49;00m options\u001b[39m.\u001b[39;49mresult_callback \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\conda\\envs\\aaa\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:66\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[1;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39melif\u001b[39;00m packet_callback:\n\u001b[0;32m     63\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     64\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     65\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mcallback should not be provided.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runner \u001b[39m=\u001b[39m _TaskRunner\u001b[39m.\u001b[39;49mcreate(graph_config, packet_callback)\n\u001b[0;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_mode \u001b[39m=\u001b[39m running_mode\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File loading is not yet supported on Windows"
     ]
    }
   ],
   "source": [
    "# 导入手部关键点检测模型\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# 载入手势识别七分类模型\n",
    "base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
    "\n",
    "\n",
    "\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options, \n",
    "                                          num_hands=4, # 最多检测多少只手\n",
    "                                         )\n",
    "\n",
    "# def print_result(result, timestamp_ms):\n",
    "#     pass\n",
    "# options = vision.GestureRecognizerOptions(base_options=base_options, \n",
    "#                                           num_hands=4, # 最多检测多少只手\n",
    "#                                           running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,\n",
    "#                                           result_callback=print_result\n",
    "#                                          )\n",
    "\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符串大小间距比例因子\n",
    "scaler = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逐帧处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(img):\n",
    "    '''输入BGR格式的 numpy array，输出BGR格式的 numpy array'''\n",
    "    global scaler\n",
    "    \n",
    "    # 记录该帧开始处理的时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 水平翻转\n",
    "    # 水平镜像翻转图像，使图中左右手与真实左右手对应\n",
    "    # 参数 1：水平翻转，0：竖直翻转，-1：水平和竖直都翻转\n",
    "    img = cv2.flip(img, 1)\n",
    "    \n",
    "    # BGR 转 RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)\n",
    "    \n",
    "    # 手势识别七分类结果\n",
    "    recognition_result = recognizer.recognize(image)\n",
    "    \n",
    "    # recognition_result = recognizer.recognize_async(image, timestamp_ms=100)\n",
    "    \n",
    "    # 手势识别预测结果：类别与置信度\n",
    "    top_gesture = recognition_result.gestures\n",
    "    \n",
    "    # 左右手预测结果\n",
    "    handness = recognition_result.handedness\n",
    "    \n",
    "    # 手部关键点坐标（相对图像宽高归一化）\n",
    "    hand_landmarks = recognition_result.hand_landmarks\n",
    "    \n",
    "    img = image.numpy_view()\n",
    "    \n",
    "    # 在画面左上角写预测结果\n",
    "    for i in range(len(hand_landmarks)): # 遍历每一只手\n",
    "        id_str = str(i+1) # 手的ID号\n",
    "        handness_str = handness[i][0].category_name # 左右手信息\n",
    "        gesture_str = top_gesture[i][0].category_name # 手势预测类别\n",
    "        confidence = '{:.2f}'.format(top_gesture[i][0].score)\n",
    "        opencv_str = '{:<2}{:<6}{:<5}{}'.format(id_str, handness_str, confidence, gesture_str)\n",
    "        # print(opencv_str)\n",
    "\n",
    "        # 图片，添加的文字，左上角坐标，字体，字体大小，颜色，字体粗细\n",
    "        color = (0,0,255) if i % 2 == 0 else (255,0,0)\n",
    "        img = cv2.putText(img, opencv_str, (25 * scaler, 60 + 60 * scaler * (i+1)), cv2.FONT_HERSHEY_SIMPLEX, 1.25 * scaler, color, 2 * scaler)\n",
    "    \n",
    "    # 可视化手部关键点检测结果\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks])\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            img,\n",
    "            hand_landmarks_proto,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    \n",
    "    # 记录该帧处理完毕的时间\n",
    "    end_time = time.time()\n",
    "    # 计算每秒处理图像帧数FPS\n",
    "    FPS = 1/(end_time - start_time)\n",
    "\n",
    "    # 在画面上写字：图片，字符串，左上角坐标，字体，字体大小，颜色，字体粗细\n",
    "    scaler = 1 # 文字大小\n",
    "    FPS_string = 'FPS  '+str(int(FPS)) # 写在画面上的字符串\n",
    "    img = cv2.putText(img, FPS_string, (25 * scaler, 60 * scaler), cv2.FONT_HERSHEY_SIMPLEX, 1.25 * scaler, (255, 0, 255), 2 * scaler)\n",
    "    \n",
    "    img = img[:,:,::-1] # RGB 转 BGR\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用摄像头实时画面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用摄像头逐帧实时处理模板\n",
    "# 不需修改任何代码，只需修改process_frame函数即可\n",
    "# 同济子豪兄 2021-7-8\n",
    "\n",
    "# 导入opencv-python\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# 获取摄像头，传入0表示获取系统默认摄像头\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# 打开cap\n",
    "cap.open(0)\n",
    "\n",
    "# 无限循环，直到break被触发\n",
    "while cap.isOpened():\n",
    "    \n",
    "    # 获取画面\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if not success: # 如果获取画面不成功，则退出\n",
    "        print('获取画面不成功，退出')\n",
    "        break\n",
    "    \n",
    "    ## 逐帧处理\n",
    "    frame = process_frame(frame)\n",
    "    \n",
    "    # 展示处理后的三通道图像\n",
    "    cv2.imshow('my_window',frame)\n",
    "    \n",
    "    key_pressed = cv2.waitKey(60) # 每隔多少毫秒毫秒，获取键盘哪个键被按下\n",
    "    # print('键盘上被按下的键：', key_pressed)\n",
    "\n",
    "    if key_pressed in [ord('q'),27]: # 按键盘上的q或esc退出（在英文输入法下）\n",
    "        break\n",
    "    \n",
    "# 关闭摄像头\n",
    "cap.release()\n",
    "\n",
    "# 关闭图像窗口\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按`q`键或`Esc`键关闭画面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
